<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1168479-8"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-1168479-8');
    </script>
    <style>
            a {
                color: #1772d0;
                text-decoration:none;
            }
            a:focus, a:hover {
                color: #f09228;
                text-decoration:none;
            }

      html { scroll-behavior: smooth; }
      body {
        font-family: Helvetica, Helvetica, sans-serif;
        <!-- background-color: #333;
        color: white-->}
      p {line-height: 26px;}
      main.container {max-width: 960px;}
      span.p-title {font-size: 16px;}
      span.p-authors {font-style: normal;font-size: 16px;}
      span.p-conference {font-style: italic;;font-size: 16px;}
      .p-bibtex {font-size: 12px; margin-bottom: 0px; max-width: 650px; text-align: left;}
      img.p-teaser {width: 200px;}
      img.profile {
        width: 160px;
        height: 160px;
        border: 1px solid #ddd;
        padding: 5px;
        box-shadow: 0 -1px 5px 1px rgba(200, 200, 200, 0.5);
      }
    </style>

    <title>Xin Yao</title>
  </head>

  <body>
    <main class="container">
      <div class="mt-5 mb-5 text-center">
          <div class="text-center text-sm-center">
            <h2>Machine Learning Methodologies for Airfare Prediction </h2>
        
	    <br>
          </div>
      </div>

      <h4 class="mb-3 text-center text-sm-left">Abstract</h4>

    <p>
      With the booming tourism industry, more and more people are choosing airplanes as a means of transportation for long-distance travel. Accurate low-price forecasting of air tickets helps the aviation industry to match demand and supply flexibly and make full use of aviation resources. Airline companies use dynamic pricing strategies to determine the price of airline tickets to maximize profits when selling airline tickets. Passengers who choose airplanes as a means of transportation want to purchase tickets at the lowest selling price for the flight of their choice. However, airline tickets are a special commodity that is time-sensitive and scarce, and the price of airline tickets is affected by various factors, such as the departure time of the plane, the number of hours of advance purchase, and the airline flight, so it is difficult for consumers to know the best time to buy a ticket. Deep learning algorithms have made great achievements in various fields in recent years, however, most prior work on airfare prediction problems is based on traditional machine learning methods, thus the performance of deep learning on this problem remains unclear. In this thesis, we did a systematic comparison of various traditional machine learning methods (i.e., Ridge Regression, Lasso Regression, K-Nearest Neighbor, Decision Tree, XGBoost, Random Forest) and deep learning methods (e.g., Fully Connected Networks, Convolutional Neural Networks, Transformer) on the problem of airfare prediction. Inspired by the observation that ensemble models like XGBoost and Random Forest achieve better performance than other traditional machine learning methods, we proposed a Bayesian neural network for airfare prediction, which is the first method that utilizes Bayesian Inference for the airfare prediction task. We evaluate the performance of different methods on an open dataset of 10,683 domestic routes in India from March 2019 to June 2019. The experimental results show that deep learning-based methods achieve better results than traditional methods in RMSE and R2, while Bayesian neural networks can achieve better performance than other machine learning methods.
    </p>
  
    <h4 class="mb-3 text-center text-sm-left">Dataset</h4>
    <p>
      The dataset used in this paper is from Kaggle, which contains a total of 10,683 routes between these cities within India: New Delhi, Bangalore, Cochin, Kolkata, Hyderabad, and Delhi from March 2019 to June 2019, and from this data, each raw data contains 11 fields of information, as shown in the chart.
    </p>
    <div class="text-center">
      <img src="data.png" class="mr-15 p-teaser mb-sm-5" style="width:100%;" />
    </div>


    <h4 class="mb-3 text-center text-sm-left">Methods</h4>
    <h4 class="mb-3 text-center text-sm-left">Ridge Regression</h4>
    <p>
      In this paper, we set a set of parameters: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000] in the ridge regression model to perform the airline ticket prediction task, using the Grid search with validation method, setting K = 10. It doesn’t work well in our dataset, we can see it in table in the chapter "Comparison of different methods".
    </p>

    <h4 class="mb-3 text-center text-sm-left">Lasso Regression</h4>
    <p>
      Similar to what we did in the Ridge regression model, we set a set of param- eters: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000] in the lasso regression model to perform the airline ticket prediction task, using the Grid search with validation method, setting K = 10. The performance is similar to the Ridge regression model, it doesn’t work well in our dataset, as we can see in table in the chapter "Comparison of different methods".
    </p>

    <h4 class="mb-3 text-center text-sm-left">K-Nearest Neighbors</h4>
    <p>
      In this study, we establish the number of neighbors in the KNN regression model, starting at 1, and enable one more nearest neighbor to be added each time K grows in value by 1 until it reaches 30 for the ticket prediction job. The outcomes are as follows when there are 10 validations.
    </p>

    <h4 class="mb-3 text-center text-sm-left">Support Vector Regression</h4>
    <p>
      In this paper, we set a set of regularization parameters C: [1e0, 1e1, 1e2, 1e3] in the support vector regression model to perform the airline ticket prediction task, using the Grid search with validation method. The strength of the regularization is inversely proportional to C and must be strictly positive. The penalty is a squared l2 penalty. And we also use a set of Gaussian kernel coefficients in our model, to get good performance. This model is time-consuming but it doesn’t work well in our dataset, we can see it in table in the chapter "Comparison of different methods".
    </p>

    <h4 class="mb-3 text-center text-sm-left">XGBoost</h4>
    <p>
      In this study, we set the depth of the tree to be 3, the learning rate of the model generated by each iteration is 0.1, the number of sub-models is 100, and the loss function is set to squared loss.
    </p>

    <h4 class="mb-3 text-center text-sm-left">Decision Tree</h4>
    <p>
      In the Decision tree regression model, we used the mean squared error function to measure the quality of a split, which is equal to variance reduction as a feature selection criterion and minimizes the L2 loss using the mean of each terminal node. And we used the “best” strategy to choose the split at each node. Meanwhile, when we set the maximum depth of the tree, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. And then we considered min_samples_split as the minimum number required to split an internal node.
    </p>

    <h4 class="mb-3 text-center text-sm-left">Random Forest Regression</h4>
    <p>
      Here, we use the random size search with a cross-validation approach to dis- cover the best parameters, set the number of cross-validations to 5, and utilize these parameters to construct a random forest regression model. The minimum number of samples needed to split an internal node is set to [2, 5, 10], while the minimum number of samples needed to be at a leaf node is set to [1, 2, 4]. We set the number of trees in the forest to [100, 200, 300, 400, 500].
    </p>

    <h4 class="mb-3 text-center text-sm-left">Fully Connected Neural Network</h4>
    <p>
      In this part, we built a fully connected neural network, which contains seven linear layers, each of which is followed by batch normalization and ReLu. There are thirteen features in the airfare dataset, so we set the number of neurons in the input layer is thirteen. And the number of neurons in the hidden layer and output layer are 1024 and 1 respectively. We set the batch size as 128. The performance is as good as the random forest regression model’s.
    </p>
    <div class="text-center">
      <img src="FCNN.png" class="mr-15 p-teaser mb-sm-5" style="width: 60%;" />
    </div>

    <h4 class="mb-3 text-center text-sm-left">Convolutional Neural Network</h4>
    <p>
      In this part, we built a convolutional neural network, which contains seven convo- lutional layers, each of which is followed by batch normalization and ReLu. There are thirteen features in the airfare dataset, so we set the number of neurons in the input layer is thirteen. And the number of neurons in the hidden layer and output layer are 1024 and 1 respectively. We set the batch size as 128.
    </p>
    <div class="text-center">
      <img src="CNN.png" class="mr-15 p-teaser mb-sm-5" style="width: 60%;" />
    </div>


    <h4 class="mb-3 text-center text-sm-left">Transformer</h4>
    <p>
      In this thesis, we design a Transformer model which has 3-layer self-attention layers and set the number of heads as 4. Since our input data do not contain sequential information, we replicate it to a dimension of 16 to simulate series data. Next, we apply linear embedding to input data to enrich the features from 13 to 256. Then we feed these sequential high-dimensional features into the Transformer encoder, which is followed by a global average pooling to get the global features among the sequential features. Finally, we apply two fully connected layers, with the number of output nodes as 256 and 1, to get the final prediction.
    </p>

    <h4 class="mb-3 text-center text-sm-left">Beyesian Neural Network</h4>
    <p>
      In this part, we built a bayesian fully connected neural network, which contains two bayesian layers and five linear layers, each of which is followed by batch nor- malization and ReLu. There are thirteen features in the airfare dataset, so we set the number of neurons in the input layer is thirteen. And the number of neurons in the hidden layer and output layer are 1024 and 1 respectively. We set the batch size as 128.
    </p>
    <div class="text-center">
      <img src="BFCNN.png" class="mr-15 p-teaser mb-sm-5" style="width: 60%;" />
    </div>
    <p>
      Meanwhile, we built a Bayesian Convolutional Neural Network, which contains one bayesian layer and six convolutional layers, each of which is followed by batch normalization and ReLu. There are thirteen features in the airfare dataset, so we set the number of neurons in the input layer is thirteen. And the number of neurons in the hidden layer and output layer are 1024 and 1 respectively. We set the batch size to 128.
    </p>
    <div class="text-center">
      <img src="BCNN.png" class="mr-15 p-teaser mb-sm-5" style="width: 60%;" />
    </div>







    <h4 class="mb-3 text-center text-sm-left">Experimental Results</h4>
    <h4 class="mb-3 text-center text-sm-left">Comparison of different methods</h4>
    <p>
      This table shows the numerical results of mean squared error(RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), and coefficient of deter- mination R2 among different methods. Specifically, we implement representative traditional machine learning methods (Lasso Regression, Ridge Regression, Support Vector Regression, K-Nearest Neighbors, XGBoost, Decision Tree, Random Forest) and deep neural networks (Transformer, Fully Connected Network, Bayesian Fully Connected Network, Convolutional Neural Network, Bayesian Convolutional Neural Network).    
    </p>
    <p>
      Comparison of different methods on RMSE, MAE, MAPE, and R2. The best two results are highlighted in red and blue.
    </p>
    <div class="text-center"> 
        <img src="table.png" class="mr-15 p-teaser mb-sm-5" style="width: 60%;" />
    </div>
    <p>
      It can be observed that for traditional machine learning methods, Decision Tree, XGBoost, and Random Forest achieve significantly better performance than Lasso Regression, Ridge Regression, Support Vector Regression, and KNN. Besides, Random Forest achieves the best performance in all metrics among all traditional machine learning methods.
    </p>
    <p>
      For the deep learning-based methods, the performance of Fully Connected Network, Bayesian FCN, Convolutional Neural Network, and Bayesian CNN achieve better performance than all traditional methods in RMSE and R2. We must highlight that with our proposed Bayesian layers, the performance of CNN and FCN can be both improved. Although Transformer is a recently popular method in various fields, the performance is the worst among all deep learning methods, which means it is not a ideal candidate for such an airfare prediction task. Besides, it should be noticed that Random Forest, as a traditional method, still achieves the best performance in MAE and MAPE among all the methods, which means the traditional machine learning methods is still playing an important role in our task.
    </p>


    <h4 class="mb-3 text-center text-sm-left">Ablation studies</h4>
    <p>
      In order to investigate how much each input feature can affect the final prediction effectively and efficiently, we do ablation studies by removing each input feature respectively. We give the RMSE and MAE results of Random Forest and Convolu- tional Neural Networks.
    </p>
    <p>
      From the data in the table below, we can see that after removing some features, the regression performance becomes better, such as "Route" and "Duration". However, for other features, the performance after deleting them is not as good as retaining all features. In this paper, we consider the case of retaining all features.    </p>
    <div class="text-center">
      <img src="Ablation.png" class="mr-15 p-teaser mb-sm-5" style="width: 60%;" />
    </div>




    <h4 class="mb-3 text-center text-sm-left">Running time comparisons</h4>
    <p>
      We also give comparisons of running time for different methods. Since the tradi- tional methods and deep learning methods are run on CPU and GPU separately, we also give the results separately.    
    </p>
    <p>
      This table shows the running times of different machine learning methods. From the data in the table, we can see that the Decision Tree Regression Model runs the fastest, and the Random Forest Regression Model takes the longest time.
    </p>
    <div class="text-center">
      <img src="runningtime.png" class="mr-15 p-teaser mb-sm-5" style="width: 45%;" />
    </div>
    <p>
      This table shows the running time of different deep learning methods. From the data in the table, we can see that the Fully Connected Network runs the fastest and has the least number of parameters; the Bayesian Convolutional Neural Network takes the longest time and has the largest number of parameters.
    </p>
    <div class="text-center">
      <img src="time.png" class="mr-15 p-teaser mb-sm-5" style="width: 60%;" />
    </div>
  

    <h4 class="mb-3 text-center text-sm-left">Conclusion</h4>
    <p>
      In this thesis, we did a systematic comparison of traditional machine learning methods (e.g., Ridge Regression, K-Nearest Neighbor, Random Forest) and deep learning methods (e.g., fully connected networks, convolutional neural networks) on the problem of airfare prediction. We proposed a Bayesian neural network for airfare prediction, which is the first method that utilizes Bayesian Inference for the airfare prediction task. We evaluate the performance of different methods on an open dataset of 10,683 domestic routes in India from March 2019 to June 2019. The experimental results show that deep learning-based methods achieve better results than traditional methods, while Bayesian neural networks can achieve better performance than other machine learning methods.
    </p>
    <p>
      This thesis can be further extended for future work. First, the adpoted public dataset only contains limited data, which hindered the performance of neural networks. It will be more worthwhile to collect a larger and wider dataset to explore the potential of deep neural networks. Second, it will be interesting to utilize the time-series information to make better prediction. Third, it is promising to design special network to better capture useful features and information from the given data.    </p>


    </main>





    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  </body>
</html>
